# Интеграция экономической системы в проект Unity и обучение ML-Agent. [in GameDev]
Отчет по лабораторной работе #5 выполнил:
- Кутявин Данил Сергеевич
- РИ-210945
Отметка о выполнении заданий (заполняется студентом):

| Задание | Выполнение | Баллы |
| ------ | ------ | ------ |
| Задание 1 | * | 60 |
| Задание 2 | * | 20 |
| Задание 3 | * | 20 |

знак "*" - задание выполнено; знак "#" - задание не выполнено;

Работу проверили:
- к.т.н., доцент Денисов Д.В.
- к.э.н., доцент Панов М.А.
- ст. преп., Фадеев В.О.

[![N|Solid](https://cldup.com/dTxpPi9lDf.thumb.png)](https://nodesource.com/products/nsolid)

[![Build Status](https://travis-ci.org/joemccann/dillinger.svg?branch=master)](https://travis-ci.org/joemccann/dillinger)

Структура отчета

- Данные о работе: название работы, фио, группа, выполненные задания.
- Цель работы.
- Задание 1.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Задание 2.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Задание 3.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Выводы.
- ✨Magic ✨

## Цель работы
### Освоить применение направления науки о данных в играх и интерактивных приложениях.
Я запустил обучение ML-агента, как показано в методичке
![Снимок экрана (125)](https://user-images.githubusercontent.com/103362515/205159571-693f11d1-51da-4e98-b6be-7b3555dd0208.png)
![Снимок экрана (126)](https://user-images.githubusercontent.com/103362515/205159646-c5ee3612-fbaa-4d56-ba24-240633d7a60b.png)
![Снимок экрана (127)](https://user-images.githubusercontent.com/103362515/205159683-2dd861f6-4996-462f-80d8-6196ece1e80c.png)

## Задание 1
### Измените параметры файла. yaml-агента и определить какие параметры и как влияют на обучение модели.
### Ход работы:
При изменении параметров: strength, checkpoint_interval, max_steps, gamma меняют исход обучения модели.

![Снимок экрана (129)](https://user-images.githubusercontent.com/103362515/205160046-22732d90-6270-425d-a874-7ce4a9d1cc06.png)
![Снимок экрана (130)](https://user-images.githubusercontent.com/103362515/205160083-fad69740-0289-4d26-b309-912f76803dc7.png)
![Снимок экрана (133)](https://user-images.githubusercontent.com/103362515/205160135-1d756eca-553d-4733-916d-b379bd2b52ea.png)
![Снимок экрана (138)](https://user-images.githubusercontent.com/103362515/205160194-c8bbfd6a-6cd9-4b40-9512-4cd19006e768.png)
![Снимок экрана (137)](https://user-images.githubusercontent.com/103362515/205160208-5c019c75-2388-4a7f-961e-2ce5b191ae6b.png)
 Но в основном на обучение модели влияет параметр strength.
 
## Задание 2
### Опишите результаты, выведенные в TensorBoard.
### Ход работы:
Environment: в этом разделе показано, как агент проявляется себя в среде в целом.

    Cumulative Reward: это общее вознаграждение, которое максимизирует агент. Обычно нужно, чтобы оно увеличивалось, но по некоторым причинам оно может и уменьшаться.
    
    Episode Length: если это значение уменьшается, то обычно это хороший знак. В конечном итоге, чем короче эпизоды, тем больше обучения.
    

Losses: в этом разделе показаны графики, представляющие вычисленные потери или затраты для политики и значения.

    Policy Loss: этот график определяет величину изменения политики со временем.

    Value Loss: это средняя потеря функции значения.

## Выводы
Я познакомился с понятием перцептрон, поработал над его обучением для вычислений логических операций. Показал на графиках зависимость количества шагов обучения перцептрона к количеству его ошибок на стадиях развития.

## Powered by

**BigDigital Team: Denisov | Fadeev | Panov**
